---
title: "Uncanny Valley - Manuscript"
author: "Kimberly A. Brink"
date: "June 30, 2016"
output: html_document
---
###Survey Questions

Uncanny Valley 1: Do you feel the robot is creepy?

Uncanny Valley 2: Does the robot make you feel weird?

Uncanny Valley 3: Would you want to play with the robot?

Agency 1: Can the robot do things on purpose?

Agency 2: When the robot moves does it choose to move?

Internal State 1: Does the robot think for itself?

Internal State 2: Does the robot know the difference between good and bad?

Internal State 3: Would the robot feel pain?

Internal State 4: Does the robot have feelings?

Internal State 5: Would the robot feel scared?

Internal State 6: Would the robot feel hungry?

Human-likeness 1: Is this robot like a human?

Exploratory 1: Does the robot know it's a robot?

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path='Manuscript Figures/', dev=c('png','postscript'), fig.width = 8, fig.height = 8, fig.keep='last', echo=FALSE, warning=FALSE, message=FALSE)
```

```{r load_libraries}
library(lubridate) #for calculating ages
library(Hmisc) #for importing SPSS
library(plyr) #for joining data frames
library(pander) #for displaying tables
library(bda) #for mediation testing
library(car) #anova
library(QuantPsyc)
library(lsr)
library(reshape2) #melt
library(lavaan)

#library(extrafont)
#library(showtext)
#font_import(pattern="[T/t]w Cen MT",prompt=FALSE)
#loadfonts(device="postscript")

#font.add("Tw Cen MT", regular = "Tw Cen MT.ttf",
#    bold = "Tw Cen MT Bold.ttf", italic = "Tw Cen MT Italic.ttf", bolditalic = "Tw Cen MT Bold Italic.ttf")
```

```{r standard_error}
#calculates standard error (automatically removes missing values)
s.error <- function(x) sd(x,na.rm=TRUE)/sqrt(length(x))
```

```{r safe_ifelse}
#when using the original ifelse statement, the class of the variable contained in the yes/no statements is not protected. if a variable is contained in the yes/no statement, it may be converted to an illegible numerical class. "ifelse makes factors lose their levels and Dates lose their class and only their mode (numeric) is restored"

#this safe.ifelse function came from: http://stackoverflow.com/questions/6668963/how-to-prevent-ifelse-from-turning-date-objects-into-numeric-objects

#it protects both date columns and factor columns

safe.ifelse <- function(cond, yes, no) {
      class.y <- class(yes)
      if ("factor" %in% class.y) {  
        levels.y = levels(yes)
      }
      X <- ifelse(cond,yes,no)
      if ("factor" %in% class.y) {  
        X = as.factor(X)
        levels(X) = levels.y
      } else {
        class(X) <- class.y
      }
      return(X)
}
```

```{r screeplot_factanal}
#http://www.stat.cmu.edu/~cshalizi/350/2008/lectures/14/lecture-14.pdf
screeplot.factanal <- function(fa.fit,xlab="factor",ylab="eigenvalue",...) {
	# sum-of-squares function for repeated application
	sosq <- function(v) {sum(v^2)}
	# Get the matrix of loadings
	my.loadings <- as.matrix(fa.fit$loadings)
	# Eigenvalues can be recovered as sum of
	# squares of each column
	evalues <- apply(my.loadings,2,sosq)
	plot(evalues,xlab=xlab,ylab=ylab,...)
}
```

```{r part_cor_function, echo = FALSE}
#runs a partial correlation that can handle missing values
#I think I made this. I'm sure pieces came from all over.

part.cor <- function(x, y, z){
  xres <- residuals(lm(x ~ z, na.action = na.exclude))
  yres <- residuals(lm(y ~ z, na.action = na.exclude)) 
  result <- cor.test(xres, yres) 
  m = data.frame(1)
  m$estimate = result$estimate 
  m$tvalue = result$statistic 
  m$df = result$parameter
  m$p.value = result$p.value
  return(m)
}
```

```{r plot_settings}
#change plot aesthetics here
responseAxis <- 1.3 #
responseAxis2 <- 1.9
ageAxis <- 2 
humanColor =  rgb(146/255, 87/255, 160/255)#'dimgray' #color of bars and points for human-like robot
machineColor = rgb(140/255, 197/255, 69/255)#'lightgray' #color of bars and points for machine-like robot
naoColor = rgb(248/255, 154/255, 52/255)#'white' #color of bars and points for Nao
plotFill = 'white' #background of plot
titleSize = 3 #size of main label
titleSizeSmall = 2
titleSizeExtraSmall = 1.5
font = "Georgia"
lineWidth = 5
```

```{r barByAge_plot_function}
#because I will be repeatedly creating a plot for multiple variables looking at the effect of robot and age, here is a function for it.
#the function creates a barplot with age on the x axis and response to an interview question on the y-axis. 
#the bars are then split into human-like and machine-like robot for each age group

barByAge <-function(v.back,v.front,data,title){
  #find means of var for each age group for the machine-like robot
  means.back <- aggregate(v.back~AgeGroup,data,mean) 

  #find means of variable for each age group for the human-like robot
  means.front <- aggregate(v.front~AgeGroup,data,mean) 
  
  #find standard error of var for each age group for the machine-like robot
  se.back <- aggregate(v.back~AgeGroup,data,s.error) 
  #find standard error of var for each age group for the human-like robot
  se.front <- aggregate(v.front~AgeGroup,data,s.error) 
  
  means.mat<-matrix(c(means.back$v.back,means.front$v.front),ncol=4,byrow=T)-1 #combine means into one matrix
  se.mat<-matrix(c(se.back$v.back,se.front$v.front),ncol=4,byrow=FALSE) #combine se into one matrix
  
  #label the columns by age group
  colnames(means.mat) <- c('3-5','6-8','9-11','12-18')
  #label the rows by robot type
  rownames(means.mat) <- c('Machine-like','Human-like')
  
  #produce rounded values of the matrix for displaying on the plot
  means.matR <- round(means.mat,2) 
  #determine y-axis range
  g.range = range(floor(min(means.mat)-1),ceiling(max(means.mat)+1)) 
  
  plot.new()
  barCenters <- barplot(means.mat, 
                        col=c(machineColor,humanColor), 
                        legend = rownames(means.mat), 
                        beside = TRUE, 
                        ylim=g.range, 
                        #yaxt='n',
                        las=1,
                        main=title,
                        xlab="Age (years)", 
                        cex.main = 2.5, 
                        cex.lab = responseAxis, 
                        cex.axis = responseAxis)
  #change y axis labels
  #axis(2, 
       #at = c(:3), 
       #labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
       #cex.axis = responseAxis) 
  
  #add +/- 1 standard error bars
  arrows(barCenters, 
         means.mat-se.mat, 
         barCenters, 
         means.mat+se.mat, 
         lwd = 3, 
         length=0.05, 
         angle=90, 
         code=3) 
  #print mean values above each bar
  #text(barCenters,
  #     means.mat+1.5*se.mat+.1,
  #     label=means.matR, 
  #     cex = responseAxis) 
}
```

```{r barByAgeGG_function}
barByAgeGG <-function(v.back,v.front,v.nao,data,title){
  #find means of var for each age group for the machine-like robot
  means.back <- aggregate(v.back~AgeGroup,data,mean)
  #find means of variable for each age group for the human-like robot
  means.front <- aggregate(v.front~AgeGroup,data,mean) 
  #find means of variable for each age group for the human-like robot
  means.nao <- aggregate(v.nao~AgeGroup,data,mean) 
  
  #find standard error of var for each age group for the machine-like robot
  se.back <- aggregate(v.back~AgeGroup,data,s.error) 
  #find standard error of var for each age group for the human-like robot
  se.front <- aggregate(v.front~AgeGroup,data,s.error) 
  #find standard error of var for each age group for the human-like robot
  se.nao <- aggregate(v.nao~AgeGroup,data,s.error) 
  
  means.mat<-matrix(c(means.back$v.back,means.front$v.front,means.nao$v.nao),
                    ncol=4,
                    byrow=T)-1 #combine means into one matrix
  se.mat<-matrix(c(se.back$v.back,se.front$v.front,se.nao$v.nao),
                 ncol=4,
                 byrow=FALSE) #combine se into one matrix
  
  #label the columns by age group
  colnames(means.mat) <- c('3-5','6-8','9-11','12-18')
  colnames(se.mat) <- c('3-5','6-8','9-11','12-18')

  #label the rows by robot type
  rownames(means.mat) <- c('Machine-like','Human-like','Nao')
  rownames(se.mat) <- c('Machine-like','Human-like','Nao')

  #produce rounded values of the matrix for displaying on the plot
  means.matR <- round(means.mat,2) 

  plotData = melt(means.matR)
  seData = melt(se.mat)
  
  names(plotData) = c("Robot","Age Group",title)
  names(seData) = c("Robot","Age Group","SE")
  #title = plotData$

  plotData = join(plotData,seData)
  
  p = ggplot(data = plotData, aes(x = `Age Group`, y = plotData[title], fill = Robot))
 # p = p + guides(fill=guide_legend(margin=.1))
  p = p + geom_bar(width = .65, position=position_dodge(), stat="identity")
  p = p + geom_errorbar(aes(ymin=plotData[title]-SE, ymax=plotData[title]+SE),
                        width=.2,
                        size = 2, # Width of the error bars
                        position=position_dodge(.65))
  p = p + labs(x="Age (years)",y=title)
  p = p + theme(text = element_text(size=36,family="Tw Cen MT"), 
                axis.text.x = element_text(size=24),
                axis.title.x = element_text(margin=margin(0,0,20,0)),
                axis.text.y=element_text(angle=90, hjust=.5, size=24),
                axis.title.y=element_text(margin=margin(0,20,0,0)),
                legend.background = element_rect(),
                legend.key.size = unit(2, "cm"),
                legend.position="bottom")
  p = p + scale_y_continuous(limits = c(0,3),
                         breaks=0:3,
                         labels=c('Not at all', 'A little bit', 'A medium amount', 'A lot'),
                         expand = waiver())
  p
  
}
```

```{r barByAge_3robots_function}
#because I will be repeatedly creating a plot for multiple variables looking at the effect of robot and age, here is a function for it.
#the function creates a barplot with age on the x axis and response to an interview question on the y-axis. 
#the bars are then split into human-like and machine-like robot for each age group
barByAge_3robots <-function(v.back,v.front,v.nao,data,title){
  #find means of variable for each age group for the machine-like robot
  means.back <- aggregate(v.back~AgeGroup,data,mean) 
  #find means of variable for each age group for the human-like robot
  means.front <- aggregate(v.front~AgeGroup,data,mean)
  #find means of variable for each age group for Nao
  means.nao <- aggregate(v.nao~AgeGroup,data,mean) 
  #find standard error of var for each age group for the machine-like robot
  se.back <- aggregate(v.back~AgeGroup,data,s.error)
  #find standard error of var for each age group for the human-like robot
  se.front <- aggregate(v.front~AgeGroup,data,s.error) 
  #find standard error of var for each age group for the Nao
  se.nao <- aggregate(v.nao ~AgeGroup,data,s.error) 
  
  #combine means into one matrix
  means.mat<-matrix(c(means.back$v.back,means.front$v.front,means.nao$v.nao),ncol=4,byrow=T)-1 
  #combine se into one matrix
  se.mat<-matrix(c(se.back$v.back,se.front$v.front,se.nao$v.nao),ncol=4,byrow=FALSE)
  
  #label the columns by age group
  colnames(means.mat) <- c('3-5','6-8','9-11','12-18')
  #label the rows by robot type
  rownames(means.mat) <- c('Machine-like','Human-like','Nao') 
  
  #produce rounded values of the matrix for displaying on the plot
  means.matR <- round(means.mat,2)
  
  #determine y-axis range
  g.range = range(0,ceiling(max(means.mat)+1)) 
  
  plot.new()
  par(bg = 'white')
  barCenters <- barplot(means.mat, 
                        col=c(machineColor,humanColor,naoColor), 
                        legend = rownames(means.mat), 
                        beside = TRUE, ylim=g.range, 
                        yaxt='n',las=1,main=title, 
                        xlab = "Age (years)", 
                        cex.main = 2.5, 
                        cex.lab = responseAxis, 
                        cex.axis = responseAxis,
                        bg = plotFill)
  #change y axis labels
  axis(2, 
       at = c(0:3), 
       labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
       cex.axis = responseAxis) 
  #add +/- 1 standard error bars
  arrows(barCenters, 
         means.mat-se.mat, 
         barCenters, 
         means.mat+se.mat,
         lwd = 3, 
         length=0.05, 
         angle=90, 
         code=3) 
  #print mean values above each bar
  #text(barCenters,means.mat+1.5*se.mat+.1,label=means.matR, cex = responseAxis) 
}
```

```{r load_file}
#library Hmisc
#load SPSS data file
filename <- "/Volumes/lsa-research01/ALL STUDIES/Current Studies/Uncanny Valley/1.0/UV - Data.sav"
UV.original <- spss.get(filename,datevars=c("DOB","DOT"),use.value.labels=F) #label DOB and DOT as date variables and import numerical values not string values for variables
```

```{r formatting}
#library plyr
#exclude orders 3 and where both conditions were presented together (leaving only between subjects for analysis)
UV.original <- UV.original[UV.original$Order==1|UV.original$Order==2|is.na(UV.original$Order),] 

UV.original = UV.original[which(!is.na(UV.original$SubID)),] #remove empty observations

UV.original$Age = NULL #remove experiementer entered age (will calculate later)
UV.original$AgeGroup = NULL #remove experiementer entered age (will calculate later)
UV.original$AgeYear = NULL #remove experiementer entered age (will calculate later)
UV.original$COMME0 = NULL #remove empty varaible
names(UV.original)[names(UV.original) == 'VAR00001'] <- 'Comments' #name unnamed variable
names(UV.original)[names(UV.original) == 'PA11'] <- 'PQ11' #correct name of variable

#original data set was created in long format, the following code converts it to short format
#each participant had an observation for either the machine-like robot or human-like robot, then they had a set of answers about Nao. This makes sure all answers about robots are included in one observation.

#separate answers about each robot into separate data frames
UV.Nao <- UV.original[UV.original$Condition==3,] 
UV.Front <- UV.original[UV.original$Condition==2,] 
UV.Back <- UV.original[UV.original$Condition==1,] 

#remove empty observations (that would have somehow magically slipped by when I removed them before)
UV.Nao = UV.Nao[which(!is.na(UV.Nao$SubID)),] 
UV.Front = UV.Front[which(!is.na(UV.Front$SubID)),] 
UV.Back = UV.Back[which(!is.na(UV.Back$SubID)),] 

#rename variables for consistency and differentiation for when they are joined back together
names(UV.Nao) <- c("SubID","Sex","DOB","DOT","Order","Condition","UV1.Nao","UV2.Nao","UV3.Nao","A1.Nao","A2.Nao","IS1.Nao","IS2.Nao","IS3.Nao","IS4.Nao","IS5.Nao","IS6.Nao","HL1.Nao","E1.Nao","PQ1","PQ2","PQ3","PQ4","PQ5","PQ6","PQ7","PQ8","PQ9","PQ10","PQ11","PQ12","PQ13","PQ14","PQ15a","PQ15b","PQ16","Comments")

#the variables below are typically empty for Nao and not robot specific (they are a parent survey about experience with robots and technology), they can be copied over from UV.Front and UV.Back where they are always filled in
#if these variables are left in UV.Nao they cause problems with joins later
#the same subid may have empty data for these variables for the Nao dataset but not the UV.Front or UV.Back datasets, which causes these variables to be incorrectly copied over during the join
UV.Nao=UV.Nao[,!(names(UV.Nao) %in% c("Sex","DOB","DOT","Order","PQ1","PQ2","PQ3","PQ4","PQ5","PQ6","PQ7","PQ8","PQ9","PQ10","PQ11","PQ12","PQ13","PQ14","PQ15a","PQ15b","PQ16","Comments"))]

#rename variables for consistency and differentiation for when they are joined back together
names(UV.Front) <- c("SubID","Sex","DOB","DOT","Order","Condition","UV1.Front","UV2.Front","UV3.Front","A1.Front","A2.Front","IS1.Front","IS2.Front","IS3.Front","IS4.Front","IS5.Front","IS6.Front","HL1.Front","E1.Front","PQ1","PQ2","PQ3","PQ4","PQ5","PQ6","PQ7","PQ8","PQ9","PQ10","PQ11","PQ12","PQ13","PQ14","PQ15a","PQ15b","PQ16","Comments")

#rename variables for consistency and differentiation for when they are joined back together
names(UV.Back) <- c("SubID","Sex","DOB","DOT","Order","Condition","UV1.Back","UV2.Back","UV3.Back","A1.Back","A2.Back","IS1.Back","IS2.Back","IS3.Back","IS4.Back","IS5.Back","IS6.Back","HL1.Back","E1.Back","PQ1","PQ2","PQ3","PQ4","PQ5","PQ6","PQ7","PQ8","PQ9","PQ10","PQ11","PQ12","PQ13","PQ14","PQ15a","PQ15b","PQ16","Comments")

#condition is no longer needed (this information is kept in the variable names)
UV.Nao$Condition = NULL
UV.Back$Condition = NULL
UV.Front$Condition = NULL

UV.NB <- join( UV.Back,UV.Nao ) #combine observations about Nao and observations about the machine-like robot by subject ID
UV.NF <- join( UV.Front,UV.Nao ) #combine observations about Nao and observations about the machine-like robot by subject ID
UV.Total <- merge( UV.NB,UV.NF,by="SubID",all.x=T,all.y=T ) #put all the observations together into one data frame

#copy over variables that do not have a ".x" or ".y" on their name
UV = UV.Total[c("SubID","UV1.Front","UV2.Front","UV3.Front","A1.Front","A2.Front","IS1.Front","IS2.Front","IS3.Front","IS4.Front","IS5.Front","IS6.Front","HL1.Front","E1.Front","UV1.Back","UV2.Back","UV3.Back","A1.Back","A2.Back","IS1.Back","IS2.Back","IS3.Back","IS4.Back","IS5.Back","IS6.Back","HL1.Back","E1.Back")]

#copy over variables with a ".x" or ".y" but remove that from the end of the variable name
for (var in  c("Sex","DOB","DOT","Order","UV1.Nao","UV2.Nao","UV3.Nao","A1.Nao","A2.Nao","IS1.Nao","IS2.Nao","IS3.Nao","IS4.Nao","IS5.Nao","IS6.Nao","HL1.Nao","E1.Nao","PQ1","PQ2","PQ3","PQ4","PQ5","PQ6","PQ7","PQ8","PQ9","PQ10","PQ11","PQ12","PQ13","PQ14","PQ15a","PQ15b","PQ16","Comments")) {
  
  UV[[var]] = safe.ifelse(is.na(UV.Total[[paste(var,".x",sep="")]]),
                          UV.Total[[paste(var,".y",sep="")]],
                          UV.Total[[paste(var,".x",sep="")]])
}

```

##Age Breakdown
```{r calculate_variables}
#library lubridate
#calculate age of participant in months
UV$Age = (year(as.period(interval(UV$DOB, UV$DOT)))*12) + month(as.period(interval(UV$DOB, UV$DOT))) + (day(as.period(interval(UV$DOB, UV$DOT)))/30)
UV$Age.C = as.numeric(scale(UV$Age,center=T,scale=T))

#calculate age of participant in years
UV$AgeYears = UV$Age/12

#separate into four age groups
UV$AgeGroup3 = factor(ifelse(UV$Age<96, "1", ifelse(UV$Age<144, "2", "3")))
UV$AgeGroup = factor(ifelse(UV$Age<72, "1", ifelse(UV$Age<108, "2", ifelse(UV$Age<144, "3", "4"))))
UV$AgeGroup5 = factor(ifelse(UV$Age<72, "1", ifelse(UV$Age<96, "2", ifelse(UV$Age<120, "3", ifelse(UV$Age<144, "4", "5")))))

UV$AgeGroup2 = factor(ifelse(UV$Age<mean(UV$Age), "1", "2"))

UV = UV[which(UV$AgeYears>=1),]

pander(table(UV$AgeGroup))
pander(table(UV$AgeGroup,UV$Sex))
pander(aggregate(UV$AgeYears,list(UV$AgeGroup),mean))
pander(aggregate(UV$AgeYears,list(UV$AgeGroup),min))
pander(aggregate(UV$AgeYears,list(UV$AgeGroup),max))


```

`r dim(UV)[1]` children (`r sum(UV$Sex == "1")` females), `r round(min(UV$AgeYears),0)` to `r round(max(UV$AgeYears),0)` years old (M = `r round(mean(UV$AgeYears),2)`) were recruited from a local Natural History Museum. The experimenter randomly assigned children to watch one of two videos: a video of a machine-like robot or a video of a human-like one. Children then answered a series of questions concerning their beliefs about the robot’s abilities. In the machine-like robot condition `r sum(UV$Order==1)` children watched 24 s of the robot Kaspar filmed from behind, so that only its wiring and electrical components could be seen (Fig. 1). In the human-like robot condition`r sum(UV$Order==2)` children watched 24 s of video of the robot Kaspar filmed from the front, where its humanlike face was clearly visible (Fig. 1). 

##Exploratory factor analysis for Kaspar Back
Factor analysis with nonorthogonal promax rotation
```{r EFA_Back_1}
#factor analysis with all variables for machine-like robot
Q.Back <- c("UV1.Back","UV2.Back","UV3.Back","A1.Back","A2.Back","IS1.Back","IS2.Back","IS3.Back","IS4.Back", "IS5.Back","IS6.Back","E1.Back","HL1.Back")

#remove variables incrementally that do not load or cross load and run analysis again
#Q.Back <- c("UV1.Back","UV2.Back","A2.Back","IS1.Back","IS2.Back","IS3.Back", "IS5.Back","IS6.Back","HL1.Back")

#create scree plot
efa.QB <- factanal(na.omit(UV[Q.Back]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QB)

#display factor loadings
efa.QB <- factanal(na.omit(UV[Q.Back]), 3, rotation="promax",scores="regression")
print(efa.QB, digits=2, sort=TRUE)
```

```{r EFA_Back_2}
#Run factor analysis with final set of variables
Q.Back <- c("UV1.Back","UV2.Back","A2.Back","IS1.Back","IS2.Back","IS3.Back","IS5.Back","IS6.Back")

#create scree plot
efa.QB <- factanal(na.omit(UV[Q.Back]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QB)

#display factor loadings
efa.QB <- factanal(na.omit(UV[Q.Back]), 3, rotation="promax",scores="regression")
print(efa.QB, digits=2, sort=TRUE)

#create three factors
back.factor1 <- c("IS1.Back","IS2.Back","A2.Back") #think, moral, choose 
back.factor2 <- c("IS3.Back","IS5.Back","IS6.Back") #pain,fear,hunger 
back.factor3 <- c("UV1.Back","UV2.Back")

#calculate alpha for each factor
a.back.factor1 = psych::alpha(UV[back.factor1]) #alpha = 0.77
a.back.factor2 = psych::alpha(UV[back.factor2]) #alpha = 0.73
a.back.factor3 = psych::alpha(UV[back.factor3],check.keys=TRUE) #alpha = 0.7
```

Removed E1 for cross-loading in machine-like condition. Removed A1 for not loading on any factor in the machine-like condition and for cross-loading in the human-like condition. Removed UV3 and IS4 for cross-loading in the human condition. Removed HL1 because it is not easily interpretable as an item for any of the factors.

I removed "Does the robot have feelings?" from analysis, because it often resulted in cross loading and created uninterpretable results. 

After considering Kaiser’s criterion (eigenvalues ≥ 1), a scree plot, and model fit indices, the results suggested a three factor model. According to Kaiser's criterion, 3 factors were recommended. The chi-square test of model fit supported the three factor solution, but not 2, $\chi^2$(`r efa.QB$dof`) = `r round(efa.QB$STATISTIC,2)`, $p =$ `r round(efa.QB$PVAL,2)`. The chi-square test is suitable for this sample of `r sum(UV$Order==1)` (less than 500).

This left three factors: Factor 1 (think, moral, choose) with loadings greater than 0.5, $\alpha =$ `r round(a.back.factor1$total[1],2)`, Factor 2 (pain, fear, hunger) with loadings greater than 0.4, $\alpha =$ `r round(a.back.factor2$total[1],2)`, and Factor 3 (creepy, weird) with loadings greater than 0.75,  $\alpha =$ `r round(a.back.factor3$total[1],2)`.

##Exploratory factor analysis for Kaspar Front
Factor analysis with nonorthogonal promax rotation

```{r EFA_Front_1}
#factor analysis with all variables for human-like robot
Q.Front <- c("UV1.Front","UV2.Front","UV3.Front","A1.Front","A2.Front","IS1.Front","IS2.Front","IS3.Front","IS4.Front", "IS5.Front","IS6.Front","E1.Front","HL1.Front")

#remove variables incrementally that do not load or cross load and run analysis again
#Q.Front <- c("UV1.Front","UV2.Front","A2.Front","IS1.Front","IS2.Front","IS3.Front","IS5.Front","IS6.Front","HL1.Front")

#create scree plot
efa.QF <- factanal(na.omit(UV[Q.Front]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QF)

#display factor loadings
efa.QF <- factanal(na.omit(UV[Q.Front]), 3, rotation="promax",scores="regression")
print(efa.QF, digits=2, sort=TRUE)
```

```{r EFA_Front_2}
#Run factor analysis with final set of variables
Q.Front <- c("UV1.Front","UV2.Front","A2.Front","IS1.Front","IS2.Front","IS3.Front","IS5.Front","IS6.Front")

#create scree plot
efa.QF <- factanal(na.omit(UV[Q.Front]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QF)

#display factor loadings
efa.QF <- factanal(na.omit(UV[Q.Front]), 3, rotation="promax",scores="regression")
print(efa.QF, digits=2, sort=TRUE)

#create recommended factors
front.factor1 <- c("IS3.Front","IS5.Front","IS6.Front") #pain,fear,hunger 
front.factor2 <- c("IS1.Front","A2.Front", "IS2.Front") #think, moral, choose
front.factor3 <- c("UV1.Front","UV2.Front")

#calculate alpha for each factor
a.front.factor1 = psych::alpha(UV[front.factor1]) #alpha = .86
a.front.factor2 = psych::alpha(UV[front.factor2]) #alpha = .66
a.front.factor3 = psych::alpha(UV[front.factor3]) #alpha = .75
```

After considering Kaiser’s criterion (eigenvalues ≥ 1), a scree plot, and model fit indices, the results suggested a three factor model. According to Kaiser's criterion, 5 factors are recommended. However, the fourth and fifth factors are not interpretable and only have one item. Therefore, we assessed three factors. The chi-square test of model fit supported the three factor solution, but not 2, $\chi^2$(`r efa.QF$dof`) = `r round(efa.QF$STATISTIC,2)`, $p =$ `r round(efa.QF$PVAL,2)`. The chi-square test is suitable for this sample of `r sum(UV$Order==2)` (less than 500).

This left three factors: Factor 1 (pain, fear, hunger) with loadings greater than 0.75, $\alpha =$ `r round(a.front.factor1$total[1],2)`, Factor 2 (think, choose, moral, aware) with loadings greater than 0.4, $\alpha =$ `r round(a.front.factor2$total[1],2)`, and Factor 3 (creepy, weird) with loadings greater than 0.75,  $\alpha =$ `r round(a.front.factor3$total[1],2)`.

##EFA by Age group
###EFA for front younger

```{r UV_by_age}
UV.old = UV[which(UV$Age<108),]
UV.young = UV[which(UV$Age>=108),]
```

```{r EFA_Front_younger, eval=FALSE}
#Run factor analysis with final set of variables
Q.Front <- c("UV1.Front","UV2.Front","A2.Front","IS1.Front","IS2.Front","IS3.Front","IS5.Front","IS6.Front")

#create scree plot
efa.QF <- factanal(na.omit(UV.young[Q.Front]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QF)

#display factor loadings
efa.QF <- factanal(na.omit(UV.young[Q.Front]), 3, rotation="promax",scores="regression")
print(efa.QF, digits=2, sort=TRUE)

#create recommended factors
front.factor1 <- c("IS3.Front","IS5.Front","IS6.Front") #pain,fear,hunger 
front.factor2 <- c("IS1.Front","A2.Front", "IS2.Front") #think, moral, choose
front.factor3 <- c("UV1.Front","UV2.Front")

#calculate alpha for each factor
a.front.factor1 = psych::alpha(UV.young[front.factor1]) #alpha = .86
a.front.factor2 = psych::alpha(UV.young[front.factor2]) #alpha = .66
a.front.factor3 = psych::alpha(UV.young[front.factor3]) #alpha = .75
```

###EFA for front older
```{r EFA_Front_older, eval=FALSE}
#Run factor analysis with final set of variables
Q.Front <- c("UV1.Front","UV2.Front","A2.Front","IS1.Front","IS2.Front","IS3.Front","IS5.Front","IS6.Front")

#create scree plot
efa.QF <- factanal(na.omit(UV.old[Q.Front]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QF)

#display factor loadings
efa.QF <- factanal(na.omit(UV.old[Q.Front]), 3, rotation="promax",scores="regression")
print(efa.QF, digits=2, sort=TRUE)

#create recommended factors
front.factor1 <- c("IS3.Front","IS5.Front","IS6.Front") #pain,fear,hunger 
front.factor2 <- c("IS1.Front","A2.Front", "IS2.Front") #think, moral, choose
front.factor3 <- c("UV1.Front","UV2.Front")

#calculate alpha for each factor
a.front.factor1 = psych::alpha(UV.old[front.factor1]) #alpha = .86
a.front.factor2 = psych::alpha(UV.old[front.factor2]) #alpha = .66
a.front.factor3 = psych::alpha(UV.old[front.factor3]) #alpha = .75
```

###EFA for back younger
```{r EFA_Back_younger, eval=FALSE}
#Run factor analysis with final set of variables
Q.Back <- c("UV1.Back","UV2.Back","A2.Back","IS1.Back","IS2.Back","IS3.Back","IS5.Back","IS6.Back")

#create scree plot
efa.QB <- factanal(na.omit(UV.young[Q.Back]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QB)

#display factor loadings
efa.QB <- factanal(na.omit(UV.young[Q.Back]), 3, rotation="promax",scores="regression")
print(efa.QB, digits=2, sort=TRUE)

#create three factors
back.factor1 <- c("IS1.Back","IS2.Back","A2.Back") #think, moral, choose 
back.factor2 <- c("IS3.Back","IS5.Back","IS6.Back") #pain,fear,hunger 
back.factor3 <- c("UV1.Back","UV2.Back")

#calculate alpha for each factor
a.back.factor1 = psych::alpha(UV.young[back.factor1]) #alpha = 0.77
a.back.factor2 = psych::alpha(UV.young[back.factor2]) #alpha = 0.73
a.back.factor3 = psych::alpha(UV.young[back.factor3],check.keys=TRUE) #alpha = 0.7
```

###EFA for back older
```{r EFA_Back_older, eval=FALSE}
#Run factor analysis with final set of variables
Q.Back <- c("UV1.Back","UV2.Back","A2.Back","IS1.Back","IS2.Back","IS3.Back","IS5.Back","IS6.Back")

#create scree plot
efa.QB <- factanal(na.omit(UV.old[Q.Back]), 4, rotation="promax",scores="regression")
screeplot.factanal(efa.QB)

#display factor loadings
efa.QB <- factanal(na.omit(UV.old[Q.Back]), 3, rotation="promax",scores="regression")
print(efa.QB, digits=2, sort=TRUE)

#create three factors
back.factor1 <- c("IS1.Back","IS2.Back","A2.Back") #think, moral, choose 
back.factor2 <- c("IS3.Back","IS5.Back","IS6.Back") #pain,fear,hunger 
back.factor3 <- c("UV1.Back","UV2.Back")

#calculate alpha for each factor
a.back.factor1 = psych::alpha(UV.old[back.factor1]) #alpha = 0.77
a.back.factor2 = psych::alpha(UV.old[back.factor2]) #alpha = 0.73
a.back.factor3 = psych::alpha(UV.old[back.factor3],check.keys=TRUE) #alpha = 0.7
```

##Factors
```{r Uncanny_Index, fig.width=16,fig.height=10}
#calculate creepiness factor for each robot
UV$UVindex.Back = (UV$UV1.Back+UV$UV2.Back)/2 
UV$UVindex.Front = (UV$UV1.Front+UV$UV2.Front)/2 
UV$UVindex.Nao = (UV$UV1.Nao+UV$UV2.Nao)/2

UV$UVindex.Nao.C = as.numeric(scale(UV$UVindex.Nao,center=T,scale=T))

pander(xtabs(~UV$UV1.Nao+UV$AgeGroup))
barByAge(UV$UVindex.Back,UV$UVindex.Front,UV, "Reports of Creepiness")
barByAge_3robots(UV$UVindex.Back,UV$UVindex.Front,UV$UVindex.Nao,UV,"Uncanniness")

barByAgeGG(UV$UVindex.Back,UV$UVindex.Front,UV$UVindex.Nao,UV,"Uncanniness")

t.test(UV$UVindex.Back[which(UV$AgeGroup==1|UV$AgeGroup==2)],UV$UVindex.Front[which(UV$AgeGroup==1|UV$AgeGroup==2)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup==1|UV$AgeGroup==2)],UV$UVindex.Front[which(UV$AgeGroup==1|UV$AgeGroup==2)])

t.test(UV$UVindex.Back[which(UV$AgeGroup3==1)],UV$UVindex.Front[which(UV$AgeGroup3==1)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup3==1)],UV$UVindex.Front[which(UV$AgeGroup3==1)])

t.test(UV$UVindex.Back[which(UV$AgeGroup3==2)],UV$UVindex.Front[which(UV$AgeGroup3==2)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup3==2)],UV$UVindex.Front[which(UV$AgeGroup3==2)])

t.test(UV$UVindex.Back[which(UV$AgeGroup3==3)],UV$UVindex.Front[which(UV$AgeGroup3==3)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup3==3)],UV$UVindex.Front[which(UV$AgeGroup3==3)])

#################
t.test(UV$UVindex.Back[which(UV$AgeGroup==3)],UV$UVindex.Front[which(UV$AgeGroup==3)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup==3)],UV$UVindex.Front[which(UV$AgeGroup==3)])

t.test(UV$UVindex.Back[which(UV$AgeGroup==4)],UV$UVindex.Front[which(UV$AgeGroup==4)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup==4)],UV$UVindex.Front[which(UV$AgeGroup==4)])

#################
t.test(UV$UVindex.Back[which(UV$AgeGroup5==1)],UV$UVindex.Front[which(UV$AgeGroup5==1)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup5==1)],UV$UVindex.Front[which(UV$AgeGroup5==1)])

t.test(UV$UVindex.Back[which(UV$AgeGroup5==2)],UV$UVindex.Front[which(UV$AgeGroup5==2)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup5==2)],UV$UVindex.Front[which(UV$AgeGroup5==2)])

t.test(UV$UVindex.Back[which(UV$AgeGroup5==3)],UV$UVindex.Front[which(UV$AgeGroup5==3)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup5==3)],UV$UVindex.Front[which(UV$AgeGroup5==3)])

t.test(UV$UVindex.Back[which(UV$AgeGroup5==4)],UV$UVindex.Front[which(UV$AgeGroup5==4)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup5==4)],UV$UVindex.Front[which(UV$AgeGroup5==4)])

t.test(UV$UVindex.Back[which(UV$AgeGroup5==5)],UV$UVindex.Front[which(UV$AgeGroup5==5)])
cohensD(UV$UVindex.Back[which(UV$AgeGroup5==5)],UV$UVindex.Front[which(UV$AgeGroup5==5)])

```

```{r Agency_Index,fig.width=16,fig.height=10}
#calculate agency factor for each robot (perceptions of agency)
UV$Agency.Back = (UV$IS1.Back+UV$IS2.Back+UV$A2.Back)/3
UV$Agency.Front = (UV$IS1.Front+UV$IS2.Front+UV$A2.Front)/3
UV$Agency.Nao = (UV$IS1.Nao+UV$IS2.Nao+UV$A2.Nao)/3

UV$Agency.Back.C = as.numeric(scale(UV$Agency.Back,center=T,scale=T))
UV$Agency.Front.C = as.numeric(scale(UV$Agency.Front,center=T,scale=T))

barByAge_3robots(UV$Agency.Back,UV$Agency.Front,UV$Agency.Nao,UV, "Attributions of Agency")
barByAgeGG(UV$Agency.Back,UV$Agency.Front,UV$Agency.Nao,UV,"Agency")

t.test(UV[which(UV$AgeGroup==4),]$Agency.Back,UV[which(UV$AgeGroup==4),]$Agency.Front)

```

```{r Experience_Index,fig.width=18,fig.height=8}
#calculate experience factor for each robot (perceptions of experience)
UV$Exp.Back = (UV$IS3.Back+UV$IS5.Back+UV$IS6.Back)/3
UV$Exp.Front = (UV$IS3.Front+UV$IS5.Front+UV$IS6.Front)/3
UV$Exp.Nao = (UV$IS3.Nao+UV$IS5.Nao+UV$IS6.Nao)/3

UV$Exp.Back.C = as.numeric(scale(UV$Exp.Back,center=T,scale=T))
UV$Exp.Front.C = as.numeric(scale(UV$Exp.Front,center=T,scale=T))

#UV$Exp.Back = (UV$IS3.Back+UV$IS5.Back)/2
#UV$Exp.Front = (UV$IS3.Front+UV$IS5.Front)/2

barByAge_3robots(UV$Exp.Back,UV$Exp.Front,UV$Exp.Nao,UV, "Attributions of Experience")
barByAgeGG(UV$Exp.Back,UV$Exp.Front,UV$Exp.Nao,UV,"Experience")

```

```{r Mind_Index,fig.width=18,fig.height=8}
UV$Mind.Back = (UV$IS3.Back+UV$IS5.Back+UV$IS6.Back+UV$IS1.Back+UV$IS2.Back+UV$A2.Back)/6
UV$Mind.Front = (UV$IS3.Front+UV$IS5.Front+UV$IS6.Front+UV$IS1.Front+UV$IS2.Front+UV$A2.Front)/6
UV$Mind.Nao = (UV$IS3.Nao+UV$IS5.Nao+UV$IS6.Front+UV$IS1.Nao+UV$IS2.Nao+UV$A2.Nao)/6

#UV$Mind.Back = (UV$IS3.Back+UV$IS5.Back+UV$IS1.Back+UV$IS2.Back+UV$A2.Back)/5
#UV$Mind.Front = (UV$IS3.Front+UV$IS5.Front+UV$IS1.Front+UV$IS2.Front+UV$A2.Front)/5
barByAgeGG(UV$Mind.Back,UV$Mind.Front,UV$Mind.Nao,UV,"Mind")
```

##Correlations
```{r correlations}
#assess correlations between the factors
cor.test(UV$UVindex.Back,UV$Agency.Back)

cor.test(UV$UVindex.Front,UV$Agency.Front)

cor.test(UV$UVindex.Back,UV$Exp.Back)

cor.test(UV$UVindex.Front,UV$Exp.Front)

cor.test(UV$Agency.Back,UV$Exp.Back)

cor.test(UV$Agency.Front,UV$Exp.Front)


###Old kids
UV.old = UV[which(UV$AgeYears>=9),]
UV.young = UV[which(UV$AgeYears<9),]

cor.test(UV.young$Agency.Back,UV.young$Exp.Back)

cor.test(UV.young$Agency.Front,UV.young$Exp.Front)

cor.test(UV.old$Agency.Back,UV.old$Exp.Back)

cor.test(UV.old$Agency.Front,UV.old$Exp.Front)
```

##Partial Correlations

```{r part_correlations}
#assess correlations between the factors

part.cor(UV$Agency.Back,UV$Exp.Back, UV$Age)

part.cor(UV$Agency.Front,UV$Exp.Front, UV$Age)


###Old kids
UV.old = UV[which(UV$AgeYears>=9),]
UV.young = UV[which(UV$AgeYears<9),]

part.cor(UV.young$Agency.Back,UV.young$Exp.Back, UV.young$Age)

part.cor(UV.young$Agency.Front,UV.young$Exp.Front, UV.young$Age)

part.cor(UV.old$Agency.Back,UV.old$Exp.Back, UV.old$Age)

part.cor(UV.old$Agency.Front,UV.old$Exp.Front, UV.old$Age)
```

##ANOVAs
```{r ANOVA}
#convert data back to long form *I know, shut up* so that we can have one creepiness factor and test the effect of robot and agency and experience factor in one analysis
UVindex = data.frame(SubID=c(UV$SubID, UV$SubID), 
                     Age=c(UV$Age,UV$Age), 
                     AgeYears=c(UV$AgeYears,UV$AgeYears),
                     AgeGroup=c(UV$AgeGroup,UV$AgeGroup), 
                     Robot=c(rep('Kaspar Back',dim(UV)[1]),rep('Kaspar Front',dim(UV)[1])), 
                     UVindex=c(UV$UVindex.Back,UV$UVindex.Front), 
                     Agency=c(UV$Agency.Back,UV$Agency.Front), 
                     Experience=c(UV$Exp.Back,UV$Exp.Front),
                     Mind=c(UV$Mind.Back,UV$Mind.Front)) 


#remove participants with no creepiness factor
UVindex=UVindex[which(!is.na(UVindex$UVindex)),]
#UVindex=UVindex[which(UVindex$AgeYears>=5),]

#because agency and experience index are correlated, center them for regression analyses to decrease the problem of multicollinearity
UVindex$UVindex.C = as.numeric(scale(UVindex$UVindex,center=T,scale=T))
UVindex$Agency.C = as.numeric(scale(UVindex$Agency,center=T,scale=T))
UVindex$Exp.C = as.numeric(scale(UVindex$Experience,center=T,scale=T))
UVindex$Age.C = as.numeric(scale(UVindex$Age,center=T,scale=T))
UVindex$Mind.C = as.numeric(scale(UVindex$Mind,center=T,scale=T))

#pander(summary(lm(Agency~Age+Robot+Age*Robot,
#                  data=UVindex)))
#pander(summary(lm(Experience~Age+Robot+Age*Robot,
#                  data=UVindex)))

UVindex$AgeGroup2 = ifelse(UVindex$AgeGroup==1|UVindex$AgeGroup==2,1,2)
#UVindex$AgeGroup3 = ifelse(UVindex$AgeGroup==1|UVindex$AgeGroup==2,1,UVindex$AgeGroup-1)


#pander(Anova(lm(UVindex~Robot+AgeGroup+Robot*AgeGroup,
#                    data=UVindex), type='III'))
# 
# pander(Anova(lm(UVindex~Robot+AgeGroup+Robot*AgeGroup,
#                    data=UVindex), type='II'))


#write.csv(UVindex, '~/Desktop/UVindex.csv')

pander(Anova(lm(UVindex~Robot+AgeGroup+Robot*AgeGroup,
                   data=UVindex), type='III'))

aov.out = aov(UVindex~Robot+AgeGroup+Robot*AgeGroup,
              data=UVindex)

etaSquared(aov.out, type=3, anova=T)

pander(Anova(lm(UVindex~Robot+AgeGroup2+Robot*AgeGroup2,
                   data=UVindex), type='II'))

aov.out = aov(UVindex~Robot+AgeGroup2+Robot*AgeGroup2,
                   data=UVindex)

etaSquared(aov.out, type=2, anova=T)


# pander(Anova(lm(UVindex~Robot+AgeGroup3+Robot*AgeGroup3,
#                    data=UVindex), type='III'))
# 
# pander(Anova(lm(UVindex~Robot+AgeGroup3+Robot*AgeGroup3,
#                    data=UVindex), type='II'))
# 
# pander(glm(UVindex~Robot*AgeYears,
#            data=UVindex))
# 
# pander(summary(aov(UVindex~Robot+AgeGroup+Robot*AgeGroup,
#                    data=UVindex)))
# 
# pander(summary(aov(UVindex.Nao~AgeGroup,
#                    data=UV)))
# 
# cor.test(UV$UVindex.Nao,UV$Age)
# cor.test(UV$UVindex.Front,UV$Age)
# 
# pander(summary(lm(UVindex.Nao~Age, data=UV)))
```

##Regression
```{r regressions}
#divide participants into groups based on when the Uncanny Valley effect appears (groups 3 & 4 show effect, groups 1 & 2 do not)
UVindex.old = UVindex[which(UVindex$AgeYears>=9),]
UVindex.young = UVindex[which(UVindex$AgeYears<9),]

t.test(UVindex~Robot,data=UVindex.young)
#sqrt((UVindex.young$UVindex[UVindex.young$Robot=='Kaspar Back'])^2 + ((UVindex.young$UVindex[UVindex.young$Robot=='Kaspar Front'])^2)/2)

t.test(UVindex~Robot,data=UVindex[which(UVindex$AgeGroup==3),])
#sqrt((UVindex.young$UVindex[UVindex.young$Robot=='Kaspar Back'])^2 + ((UVindex.young$UVindex[UVindex.young$Robot=='Kaspar Front'])^2)/2)

t.test(UVindex~Robot,data=UVindex[which(UVindex$AgeGroup==4),])
#sqrt((UVindex.young$UVindex[UVindex.young$Robot=='Kaspar Back'])^2 + ((UVindex.young$UVindex[UVindex.young$Robot=='Kaspar Front'])^2)/2)

#regression analyses to predict creepiness factor
lm.out = lm(UVindex~Agency.C+Exp.C+Age+Robot+Agency.C*Age+Exp.C*Age+Robot*Age,
            data=UVindex)

pander(summary(lm.out), caption = "full sample regression")

pander(lm.beta(lm.out), caption = "beta estimates of full sample regression")

#look at whether different predictors are significant for the younger children where the uncanny effect does not exist.
lm.out = lm(UVindex~Agency.C+Exp.C+Age+Robot+Agency.C*Age+Exp.C*Age+Robot*Age,
            data=UVindex.young)

pander(summary(lm.out), caption="Younger sample regression")

pander(lm.beta(lm.out), caption="Beta estimates of younger sample regression")

#look at whether different predictors are significant for the older children where the uncanny effect exists.
lm.out = lm(UVindex~Agency.C+Exp.C+Age+Robot+Agency.C*Age+Exp.C*Age+Robot*Age,
            data=UVindex.old)

pander(summary(lm.out), caption="Older sample regression")

pander(lm.beta(lm.out), caption="Beta estimates of older sample regression")


cor.test(UVindex.young$Experience,UVindex.young$Agency)
cor.test(UVindex.old$Experience,UVindex.old$Agency)

pander(summary(lm(Experience~Agency.C*Age.C,data=UVindex)))

UVindex.reg = data.frame(Experience=UVindex$Experience,Agency.C=as.numeric(UVindex$Agency.C),Age.C=as.numeric(UVindex$Age.C))
res = lm(Experience~Agency.C*Age.C,data=UVindex.reg)
pander(lm.beta(res), caption="Beta estimates for correlation and interaction")
newdf <- expand.grid(Agency.C=seq(-1,1),Age.C=seq(-1.5,2.4,.65))

yp=predict(res, newdf)

p <- ggplot(data=transform(newdf, yp), 
            aes(y=yp, x=Agency.C, color=factor(Age.C))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="Age", labels=as.character(seq(4,16,2))) + 
  labs(x="Agency", y="Experience") + 
  scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
```

```{r moderation}
#model 4
#Y = UV
#moderator = Age
#X = Robot

UVindex$AgencyXAge = UVindex$Agency.C*UVindex$Age.C
UVindex$Exp = UVindex$Experience

model <- '
  ## Direct effects
  Exp ~ c1*Agency.C
  Exp ~ c2*Age.C
  Exp ~ c3*AgencyXAge
  ## Conditional direct effect
  mod0 := c1 + c3*-1.5
  mod1 := c1 + c3*-.85
  mod2 := c1 + c3*-.2
  mod3 := c1 + c3*.45
  mod4 := c1 + c3*1.1
  mod5 := c1 + c3*1.75
  mod6 := c1 + c3*2.4
  '
         
fit <- sem(model, data = UVindex, se = "bootstrap", test = "bootstrap")
summary(fit, standardized = TRUE, rsq = TRUE)
parameterEstimates(fit)
```

```{r}
get_real <- function(coef, scaled_covariate){

            # collect mean and standard deviation from scaled covariate
            mean_sd <- unlist(attributes(scaled_covariate)[-1])

            # reverse the z-transformation
             answer <- (coef * mean_sd[2]) + mean_sd[1]

            # this value will have a name, remove it
             names(answer) <- NULL

            # return unscaled coef
              return(answer)
}

get_real(-1.5, scale(UVindex$Age,center = T, scale = T))/12
get_real(-.85, scale(UVindex$Age,center = T, scale = T))/12
get_real(-.2, scale(UVindex$Age,center = T, scale = T))/12
get_real(.45, scale(UVindex$Age,center = T, scale = T))/12
get_real(1.1, scale(UVindex$Age,center = T, scale = T))/12
get_real(1.75, scale(UVindex$Age,center = T, scale = T))/12
get_real(2.4, scale(UVindex$Age,center = T, scale = T))/12
```

```{r Uncanny_scatter}
KasparFront = UVindex[ which( UVindex$Robot=="Kaspar Front"), ]
KasparBack = UVindex[ which( UVindex$Robot=="Kaspar Back"), ]

plot.new()
par(mar=c(5,5,5,5)+0.1)
plot(KasparFront$AgeYears, 
     jitter(KasparFront$UVindex), 
     main = "Reports of Creepiness", 
     xlab = 'Age (years)', 
     ylab="", 
     yaxt = 'n', 
     col = humanColor, 
     cex.main = titleSizeSmall, 
     cex.axis = responseAxis, 
     cex.lab = responseAxis)

#label y-axis
axis(2, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)

#add regression line for human-like condition
abline(lm(UVindex~AgeYears,
          data = KasparFront),
       col = humanColor, 
       lwd = lineWidth)

#print regression equation on plot
#fit <- lm(KasparFront$UVindex~KasparFront$AgeYears)
#a <- summary(fit)$coefficient['(Intercept)','Estimate']
#b <- summary(fit)$coefficient['KasparFront$AgeYears','Estimate']
#r2 <- format(summary(fit)$adj.r.squared, digits = 3)
#f <- summary(fit)$fstatistic
#p <- format(pf(f[1],f[2],f[3],lower.tail=F), digits = 3)
#p <- ifelse(p<0.001, 0.001, ifelse(p<0.01, 0.01, ifelse(p<0.05,0.05,p)))
#text(((2.5-a)/b)-8, 2.5, bquote(paste( R^2 == .(r2), ', p '< .(p))), col = humanColor, cex = responseAxis)

#add data points for machine-Like condition to plot
points(KasparBack$AgeYears, jitter(KasparBack$UVindex), col = machineColor)
#add regression line for machine-like condition
abline(lm(UVindex~AgeYears,
          data=KasparBack), 
       col = machineColor, 
       lwd = lineWidth)

#print regression equation on plot
#fit <- lm(KasparBack$UVindex~KasparBack$AgeYears)
#a <- summary(fit)$coefficient['(Intercept)','Estimate']
#b <- summary(fit)$coefficient['KasparBack$AgeYears','Estimate']
#r2 <- format(summary(fit)$adj.r.squared, digits = 3)
#f <- summary(fit)$fstatistic
#p <- format(pf(f[1],f[2],f[3],lower.tail=F), digits = 3)
#p <- ifelse(p<0.001, 0.001, ifelse(p<0.01, 0.01, ifelse(p<0.05,0.05,p)))
#text(((2.5-a)/b)+8, 2.5, bquote(paste( R^2 == .(r2), ', p '< .(p))), col = machineColor, cex = responseAxis)

legend('topright',
       c("Human-like", "Machine-like"), 
       col = c(humanColor,machineColor), 
       lwd = lineWidth, 
       bg = 'white', 
       inset = .025)
```

##Experience as mediator
###Sobel Test for Mediation
```{r mediation_experience}
#library(bda)
#mediator variable: mv = expindex
#ndependent variable: iv = robot
#dependent variable: dv = Uncanniness

#test for replication of adult data with only older children
pander(mediation.test(mv=UVindex.old$Exp.C,
                      iv=UVindex.old$Robot,
                      dv=UVindex.old$UVindex), 
       caption="Test of experience as a mediator with robot appearance as predictor for children older than 9")

#test for replication of adult data with all children
pander(mediation.test(mv=UVindex$Exp.C,
                      iv=UVindex$Robot,
                      dv=UVindex$UVindex), 
       caption="Test of experience as a mediator for all children with robot appearance as predictor for all children")
```

###MacKinnon Mediation

```{r MacKinnon_mediation_exp}
#MacKinnon et al., 2007
#Product of coefficients methods

#Estimate Equations 2 (Y = i + c'X + bM + e) and 3 (M = i + aX + e), i = intercept, e = error, M = mediator, X = predictor, y = criterion. Compute product of a & b.

#Equation 2
eq2 = lm(UVindex~Exp.C+Robot,
         data=UVindex.old)
pander(summary(eq2), caption='Is the slope of Experience significantly different from zero?')
b = eq2$coefficients[["Exp.C"]]
var.b = (summary(eq2)$coef[,'Std. Error'][['Exp.C']])^2
n.b = length(resid(eq2))

#Equation 3
eq3 = lm(Exp.C~Robot,
         data=UVindex.old)
pander(summary(eq3), caption='Is the slope of Robot significantly different from zero?')
a = eq3$coefficients[["RobotKaspar Front"]]
var.a = (summary(eq3)$coef[,'Std. Error'][['RobotKaspar Front']])^2
n.a = length(resid(eq3))

#To test for significance, the product is then divided by the standard error of the product and the ratio is compared to a standard normal distribution.

#variance of the roduct of two independent random variables (Goodman, 1960)
var.ab = a^2*(var.b/n.b) + b^2*(var.a/n.a) - ((var.a/n.a)*(var.b/n.b)) 

#The standard error of an estimate may also be defined as the square root of the estimated error 
se.ab = sqrt(abs(var.ab))

sig = (a*b)/se.ab
abs(sig)>1.96

#The rationale behind this method is that mediation depends on the extent to which the program changes the mediator, a, and the extent to which the mediator affects the outcome variable, b.
```

The product of the coefficients ab = `r round(a*b,4)`, t = `r round(sig,2)` (MacKinnon, Fairchild, & Fritz, 2007)

##Experience as a mediator for robot when controlling for age

Step 1: Show that the causal variable is correlated with the outcome

Step 2: Show that the causal variable is correlated with the mediator

Step 3: Show that the mediator affects the outcome variable

Step 4: To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero

### Baron & Kenny Mediation
```{r BaronKenny_mediation_experience}
#Find a mediation test, where experience mediates the effect of robot type on Uncanniness while controlling for age.

#http://davidakenny.net/cm/mediate.htm
#Step 1:  Show that the causal variable is correlated with the outcome.  Use Y as the criterion variable in a regression equation and X as a predictor (estimate and test path c in the above figure). This step establishes that there is an effect that may be mediated.

pander(summary(lm(UVindex~Robot+Age.C,
                  data=UVindex)), 
       caption="Step 1: Show that the causal variable is correlated with the outcome")

#Step 2: Show that the causal variable is correlated with the mediator.  Use M as the criterion variable in the regression equation and X as a predictor (estimate and test path a).  This step essentially involves treating the mediator as if it were an outcome variable.

pander(summary(lm(Exp.C~Robot+Age.C,
                  data=UVindex)), 
       caption="Step 2: Show that the causal variable is correlated with the mediator")

#Step 3:  Show that the mediator affects the outcome variable.  Use Y as the criterion variable in a regression equation and X and M as predictors (estimate and test path b).  It is not sufficient just to correlate the mediator with the outcome because the mediator and the outcome may be correlated because they are both caused by the causal variable X.  Thus, the causal variable must be controlled in establishing the effect of the mediator on the outcome.

pander(summary(lm(UVindex~Exp.C+Robot+Age.C,
                  data=UVindex)), 
       caption="Step 3:  Show that the mediator affects the outcome variable")

#Step 4:  To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero (see discussion below on significance testing).   The effects in both Steps 3 and 4 are estimated in the same equation.
```

###MacKinnon Mediation

```{r MacKinnon_mediation_experience}
#MacKinnon et al., 2007
#Product of coefficients methods

#Estimate Equations 2 (Y = i + c'X + bM + e) and 3 (M = i + aX + e), i = intercept, e = error, M = mediator, X = predictor, y = criterion. Compute product of a & b.

#Equation 2
eq2 = lm(UVindex~Exp.C+Robot+Age.C,
         data=UVindex)
pander(summary(eq2), caption='Is the slope of Experience significantly different from zero?')
b = eq2$coefficients[["Exp.C"]]
var.b = (summary(eq2)$coef[,'Std. Error'][['Exp.C']])^2
n.b = length(resid(eq2))

#Equation 3
eq3 = lm(Exp.C~Robot+Age.C,
         data=UVindex)
pander(summary(eq3), caption='Is the slope of Robot significantly different from zero?')
a = eq3$coefficients[["RobotKaspar Front"]]
var.a = (summary(eq3)$coef[,'Std. Error'][['RobotKaspar Front']])^2
n.a = length(resid(eq3))

#To test for significance, the product is then divided by the standard error of the product and the ratio is compared to a standard normal distribution.

#variance of the roduct of two independent random variables (Goodman, 1960)
var.ab = a^2*(var.b/n.b) + b^2*(var.a/n.a) - ((var.a/n.a)*(var.b/n.b)) 

#The standard error of an estimate may also be defined as the square root of the estimated error 
se.ab = sqrt(abs(var.ab))

sig = (a*b)/se.ab
abs(sig)>1.96

#The rationale behind this method is that mediation depends on the extent to which the program changes the mediator, a, and the extent to which the mediator affects the outcome variable, b.
```

The product of the coefficients ab = `r round(a*b,4)`, t = `r round(sig,2)` (MacKinnon, Fairchild, & Fritz, 2007)

```{r scatter_mediation_experience}
#where the independent variable, X, is dichotomous (to simplify the plot), the mediator, M, is on the horizontal axis, and the dependent variable, Y, is on the vertical axis

KasparFront = UVindex[ which( UVindex$Robot=="Kaspar Front"), ]
KasparBack = UVindex[ which( UVindex$Robot=="Kaspar Back"), ]

plot.new()
par(mar=c(5,5,5,5)+0.1)
plot(KasparFront$Experience, 
     jitter(KasparFront$UVindex), 
     main = "Mediation Plot", 
     xlab = 'Experience', 
     ylab="Uncanniness", 
     yaxt = 'n', 
     xaxt = 'n', 
     col = humanColor, 
     cex.main = titleSizeSmall, 
     cex.axis = responseAxis, 
     cex.lab = responseAxis)

#add regression line for human-like condition
abline(lm(UVindex~Experience+Age,
          data = KasparFront),
       col = humanColor, 
       lwd = lineWidth)

#print x-axis labels
axis(1, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)
#print y-axis labels
axis(2, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)

#add data points for machine-Like condition to plot
points(jitter(KasparBack$Experience), jitter(KasparBack$UVindex), col = machineColor)
#add regression line for machine-like condition
abline(lm(UVindex~Experience+Age,
          data = KasparBack),
       col = machineColor, 
       lwd = lineWidth)

legend('topright',
       c("Human-like", "Machine-like"), 
       col = c(humanColor,machineColor), 
       lwd = lineWidth, 
       bg = 'white', 
       inset = .025)

#The two slanted lines in the plot represent the relation of M to Y in each X group, one line for the human-like robot group and one line for the machine-like robot group. The two lines are not parallel because there is an XM interaction in Equation 2, with the slope of each line equal to the b coefficient (b = -.0039 se(b) = .13; b = -.17; se(b) = .13). The distance between the horizontal lines in the plots is equal to the overall effect of X on Y, c (c = -.04, se(c) = .09) and the distance between the vertical lines is equal to the effect of X on M, a (a = .67, se(a) = .20). The mediated effect is the change in the regression line relating M to Y for a change in M of a units as shown in the graph. Plots of the mediated effect may be useful to investigate the distributions of data for outliers and to improve understanding of relations among variables in the mediation model.
```

##Experience as a mediator for robot when controlling for age for only older children

###Baron & Kenny Mediation
Step 1: Show that the causal variable is correlated with the outcome

Step 2: Show that the causal variable is correlated with the mediator

Step 3: Show that the mediator affects the outcome variable

Step 4: To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero

```{r BaronKenny_mediation_experience_old}
#Find a mediation test, where experience mediates the effect of robot type on Uncanniness while controlling for age.

#http://davidakenny.net/cm/mediate.htm
#Step 1:  Show that the causal variable is correlated with the outcome.  Use Y as the criterion variable in a regression equation and X as a predictor (estimate and test path c in the above figure). This step establishes that there is an effect that may be mediated.

pander(summary(lm(UVindex~Robot+Age.C,
                  data=UVindex.old)), 
       caption="Step 1: Show that the causal variable is correlated with the outcome")

#Step 2: Show that the causal variable is correlated with the mediator.  Use M as the criterion variable in the regression equation and X as a predictor (estimate and test path a).  This step essentially involves treating the mediator as if it were an outcome variable.

pander(summary(lm(Exp.C~Robot+Age.C,
                  data=UVindex.old)), 
       caption="Step 2: Show that the causal variable is correlated with the mediator")

#Step 3:  Show that the mediator affects the outcome variable.  Use Y as the criterion variable in a regression equation and X and M as predictors (estimate and test path b).  It is not sufficient just to correlate the mediator with the outcome because the mediator and the outcome may be correlated because they are both caused by the causal variable X.  Thus, the causal variable must be controlled in establishing the effect of the mediator on the outcome.

pander(summary(lm(UVindex~Exp.C+Robot+Age.C,
                  data=UVindex.old)), 
       caption="Step 3:  Show that the mediator affects the outcome variable")

#Step 4:  To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero (see discussion below on significance testing).   The effects in both Steps 3 and 4 are estimated in the same equation.


```

###MacKinnon Mediation

```{r MacKinnon_mediation_experience_old}
#MacKinnon et al., 2007
#Product of coefficients methods

#Estimate Equations 2 (Y = i + c'X + bM + e) and 3 (M = i + aX + e), i = intercept, e = error, M = mediator, X = predictor, y = criterion. Compute product of a & b.

#Equation 2
eq2 = lm(UVindex~Exp.C+Robot+Age.C,
         data=UVindex.old)
pander(summary(eq2), caption='Is the slope of Experience significantly different from zero?')
b = eq2$coefficients[["Exp.C"]]
var.b = (summary(eq2)$coef[,'Std. Error'][['Exp.C']])^2
n.b = length(resid(eq2))

#Equation 3
eq3 = lm(Exp.C~Robot+Age.C,
         data=UVindex.old)
pander(summary(eq3), caption='Is the slope of Robot significantly different from zero?')
a = eq3$coefficients[["RobotKaspar Front"]]
var.a = (summary(eq3)$coef[,'Std. Error'][['RobotKaspar Front']])^2
n.a = length(resid(eq3))

#To test for significance, the product is then divided by the standard error of the product and the ratio is compared to a standard normal distribution.

#variance of the roduct of two independent random variables (Goodman, 1960)
var.ab = a^2*(var.b/n.b) + b^2*(var.a/n.a) - ((var.a/n.a)*(var.b/n.b)) 

#The standard error of an estimate may also be defined as the square root of the estimated error 
se.ab = sqrt(abs(var.ab))

sig = (a*b)/se.ab
abs(sig)>1.96

#The rationale behind this method is that mediation depends on the extent to which the program changes the mediator, a, and the extent to which the mediator affects the outcome variable, b.
```

The product of the coefficients ab = `r round(a*b,4)`, t = `r round(sig,2)` (MacKinnon, Fairchild, & Fritz, 2007)

```{r scatter_mediation_experience_old}
#where the independent variable, X, is dichotomous (to simplify the plot), the mediator, M, is on the horizontal axis, and the dependent variable, Y, is on the vertical axis

KasparFront = UVindex.old[ which( UVindex.old$Robot=="Kaspar Front"), ]
KasparBack = UVindex.old[ which( UVindex.old$Robot=="Kaspar Back"), ]

#create scatter plot that looks at effect of age and robot type on creepiness factor
plot.new()
par(mar=c(5,5,5,5)+0.1)
plot(KasparFront$Experience, 
     jitter(KasparFront$UVindex), 
     main = "Mediation Plot", 
     xlab = 'Experience', 
     ylab="Uncanniness", 
     yaxt = 'n', 
     xaxt = 'n', 
     col = humanColor, 
     cex.main = titleSizeSmall, 
     cex.axis = responseAxis, 
     cex.lab = responseAxis)

#print x-axis labels
axis(1, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)
#print y-axis labels
axis(2, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)


#add regression line for human-like condition
abline(lm(UVindex~Experience+Age, 
          data=KasparFront), 
       col = humanColor, 
       lwd = lineWidth)

#add data points for machine-Like condition to plot
points(jitter(KasparBack$Experience), jitter(KasparBack$UVindex), col = machineColor)
#add regression line for machine-like condition
abline(lm(UVindex~Experience+Age,
          data=KasparBack), 
       col = machineColor, 
       lwd = lineWidth)

legend('topright',
       c("Human-like", "Machine-like"), 
       col = c(humanColor,machineColor), 
       lwd = lineWidth, 
       bg = 'white', 
       inset = .025)

#The two slanted lines in the plot represent the relation of M to Y in each X group, one line for the human-like robot group and one line for the machine-like robot group. The two lines are not parallel because there is an XM interaction in Equation 2, with the slope of each line equal to the b coefficient (b = -.0039 se(b) = .13; b = -.17; se(b) = .13). The distance between the horizontal lines in the plots is equal to the overall effect of X on Y, c (c = -.04, se(c) = .09) and the distance between the vertical lines is equal to the effect of X on M, a (a = .67, se(a) = .20). The mediated effect is the change in the regression line relating M to Y for a change in M of a units as shown in the graph. Plots of the mediated effect may be useful to investigate the distributions of data for outliers and to improve understanding of relations among variables in the mediation model.
```

##Agency as mediator
###Sobel Test for Mediation
```{r mediation_agency}
UVindex.ag = UVindex[which(!is.na(UVindex$Agency)),]

pander(mediation.test(mv=UVindex.ag$Agency,
                      iv=UVindex.ag$Robot,
                      dv=UVindex.ag$UVindex), 
       caption="Test of Agency as a mediator with robot appearance as predictor for all children")

pander(mediation.test(mv=UVindex.ag$Agency,
                      iv=UVindex.ag$Age,
                      dv=UVindex.ag$UVindex), 
       caption="Test of Agency as a mediator with age as predictor for all children")
```

##Agency as a mediator of robot when controlling for age

Step 1: Show that the causal variable is correlated with the outcome

Step 2: Show that the causal variable is correlated with the mediator

Step 3: Show that the mediator affects the outcome variable

Step 4: To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero

###Baron & Kenny Mediation

```{r BaronKenny_mediation_agency}
#Find a mediation test, where experience mediates the effect of robot type on Uncanniness while controlling for age.

#http://davidakenny.net/cm/mediate.htm
#Step 1:  Show that the causal variable is correlated with the outcome.  Use Y as the criterion variable in a regression equation and X as a predictor (estimate and test path c in the above figure). This step establishes that there is an effect that may be mediated.

pander(summary(lm(UVindex~Robot+Age.C,
                  data=UVindex)), 
       caption="Step 1: Show that the causal variable is correlated with the outcome")

#Step 2: Show that the causal variable is correlated with the mediator.  Use M as the criterion variable in the regression equation and X as a predictor (estimate and test path a).  This step essentially involves treating the mediator as if it were an outcome variable.

pander(summary(lm(Agency.C~Robot+Age.C,
                  data=UVindex)), 
       caption="Step 2: Show that the causal variable is correlated with the mediator")

#Step 3:  Show that the mediator affects the outcome variable.  Use Y as the criterion variable in a regression equation and X and M as predictors (estimate and test path b).  It is not sufficient just to correlate the mediator with the outcome because the mediator and the outcome may be correlated because they are both caused by the causal variable X.  Thus, the causal variable must be controlled in establishing the effect of the mediator on the outcome.

pander(summary(lm(UVindex~Agency.C+Robot+Age.C,
                  data=UVindex)), 
       caption="Step 3:  Show that the mediator affects the outcome variable")

#Step 4:  To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero (see discussion below on significance testing). The effects in both Steps 3 and 4 are estimated in the same equation.
```

###MacKinnon Mediation

```{r MacKinnon_mediation_agency}
#MacKinnon et al., 2007

#Estimate Equations 2 (Y = i + c'X + bM + e) and 3 (M = i + aX + e), i = intercept, e = error, M = mediator, X = predictor, y = criterion. Compute product of a & b.

#Equation 2
eq2 = lm(UVindex~Robot+Agency.C+Age.C,
         data=UVindex)
pander(summary(eq2))
pander(summary(eq2), caption='Is the slope of Agency significantly different from zero?')
b = eq2$coefficients[["Agency.C"]]
var.b = (summary(eq2)$coef[,'Std. Error'][['Agency.C']])^2
n.b = length(resid(eq2))

#Equation 3
eq3 = lm(Agency.C~Robot+Age.C,
         data=UVindex)
pander(summary(eq3), caption='Is the slope of Robot significantly different from zero?')
a = eq3$coefficients[["RobotKaspar Front"]]
var.a = (summary(eq3)$coef[,'Std. Error'][['RobotKaspar Front']])^2
n.a = length(resid(eq3))

#To test for significance, the product is then divided by the standard error of the product and the ratio is compared to a standard normal distribution.

#variance of the roduct of two independent random variables (Goodman, 1960)
var.ab = a^2*(var.b/n.b) + b^2*(var.a/n.a) - ((var.a/n.a)*(var.b/n.b)) 

#The standard error of an estimate may also be defined as the square root of the estimated error 
se.ab = sqrt(abs(var.ab))

sig = (a*b)/se.ab
abs(sig)>1.96

#The rationale behind this method is that mediation depends on the extent to which the program changes the mediator, a, and the extent to which the mediator affects the outcome variable, b.
```

The product of the coefficients ab = `r round(a*b,4)`, t = `r round(sig,2)` (MacKinnon, Fairchild, & Fritz, 2007)

```{r scatter_mediation_agency}
#where the independent variable, X, is dichotomous (to simplify the plot), the mediator, M, is on the horizontal axis, and the dependent variable, Y, is on the vertical axis

KasparFront = UVindex[ which( UVindex$Robot=="Kaspar Front"), ]
KasparBack = UVindex[ which( UVindex$Robot=="Kaspar Back"), ]

#create scatter plot that looks at effect of age and robot type on creepiness factor
plot.new()
par(mar=c(5,5,5,5)+0.1)
plot(jitter(KasparFront$Agency), 
     jitter(KasparFront$UVindex), 
     main = "Reports of Creepiness", 
     xlab = 'Agency', 
     ylab="Uncanniness", 
     yaxt = 'n', 
     xaxt = 'n', 
     col = humanColor, 
     cex.main = titleSizeSmall, 
     cex.axis = responseAxis, 
     cex.lab = responseAxis)

axis(1, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)
axis(2, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)

abline(lm(UVindex~Agency+Age,
          data = KasparFront),
       col = humanColor, 
       lwd = lineWidth)

points(jitter(KasparBack$Agency), jitter(KasparBack$UVindex), col = machineColor)
abline(lm(UVindex~Agency+Age,
          data = KasparBack), 
       col = machineColor, 
       lwd = lineWidth)

legend('topright',
       c("Human-like", "Machine-like"), 
       col = c(humanColor,machineColor), 
       lwd = lineWidth, 
       bg = 'white', 
       inset = .025)

#The two slanted lines in the plot represent the relation of M to Y in each X group, one line for the human-like robot group and one line for the machine-like robot group. The two lines are not parallel because there is an XM interaction in Equation 2, with the slope of each line equal to the b coefficient (b = -.10 se(b) = .10; b = -.37 se(b) = .11). The distance between the horizontal lines in the plots is equal to the overall effect of X on Y, c (c = -.06, se(c) = .002) and the distance between the vertical lines is equal to the effect of X on M, a (a = .28, se(a) = .13). The mediated effect is the change in the regression line relating M to Y for a change in M of a units as shown in the graph. Plots of the mediated effect may be useful to investigate the distributions of data for outliers and to improve understanding of relations among variables in the mediation model.
```

## Mind as mediator
###Sobel Test for Mediation
```{r mediation_mind}
#library(bda)
#mediator variable: mv = mind
#ndependent variable: iv = robot
#dependent variable: dv = Uncanniness

#test for replication of adult data with only older children
pander(mediation.test(mv=UVindex.old$Mind.C,
                      iv=UVindex.old$Robot,
                      dv=UVindex.old$UVindex), 
       caption="Test of Mind as a mediator with robot appearance as predictor for children older than 9")

#test for replication of adult data with all children
#pander(mediation.test(mv=UVindex$Mind.C,
#                      iv=UVindex$Robot,
#                      dv=UVindex$UVindex), 
#       caption="Test of Mind as a mediator for all children with robot appearance as predictor for all children")
```

###MacKinnon Mediation
```{r MacKinnon_mediation_mind}

mind.back.factor=c("IS1.Back","IS2.Back","A2.Back","IS3.Back","IS5.Back","IS6.Back") #pain,fear,hunger 
psych::alpha(UV[mind.back.factor])

mind.front.factor=c("IS1.Front","IS2.Front","A2.Front","IS3.Front","IS5.Front","IS6.Front") #pain,fear,hunger 
psych::alpha(UV[mind.front.factor])

mind.factor=c("Agency","Experience")
psych::alpha(UVindex[mind.factor])

cor.test(UV$Mind.Front,UV$Age)
#MacKinnon et al., 2007
#Product of coefficients methods

#Estimate Equations 2 (Y = i + c'X + bM + e) and 3 (M = i + aX + e), i = intercept, e = error, M = mediator, X = predictor, y = criterion. Compute product of a & b.

#Equation 2
eq2 = lm(UVindex~Mind.C+Robot,
         data=UVindex.old)
pander(summary(eq2), caption='Is the slope of Mind significantly different from zero?')
b = eq2$coefficients[["Mind.C"]]
var.b = (summary(eq2)$coef[,'Std. Error'][['Mind.C']])^2
n.b = length(resid(eq2))

#Equation 3
eq3 = lm(Mind.C~Robot,
         data=UVindex.old)
pander(summary(eq3), caption='Is the slope of Robot significantly different from zero?')
a = eq3$coefficients[["RobotKaspar Front"]]
var.a = (summary(eq3)$coef[,'Std. Error'][['RobotKaspar Front']])^2
n.a = length(resid(eq3))

#To test for significance, the product is then divided by the standard error of the product and the ratio is compared to a standard normal distribution.

#variance of the roduct of two independent random variables (Goodman, 1960)
var.ab = a^2*(var.b/n.b) + b^2*(var.a/n.a) - ((var.a/n.a)*(var.b/n.b)) 

#The standard error of an estimate may also be defined as the square root of the estimated error 
se.ab = sqrt(abs(var.ab))

sig = (a*b)/se.ab
abs(sig)>1.96

#The rationale behind this method is that mediation depends on the extent to which the program changes the mediator, a, and the extent to which the mediator affects the outcome variable, b.
```

The product of the coefficients ab = `r round(a*b,4)`, t = `r round(sig,2)` (MacKinnon, Fairchild, & Fritz, 2007)

##Mind as a mediator for robot when controlling for age

### Baron & Kenny Mediation
Step 1: Show that the causal variable is correlated with the outcome

Step 2: Show that the causal variable is correlated with the mediator

Step 3: Show that the mediator affects the outcome variable

Step 4: To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero

```{r BaronKenny_mediation_mind}
#Find a mediation test, where experience mediates the effect of robot type on Uncanniness while controlling for age.

#http://davidakenny.net/cm/mediate.htm
#Step 1:  Show that the causal variable is correlated with the outcome.  Use Y as the criterion variable in a regression equation and X as a predictor (estimate and test path c in the above figure). This step establishes that there is an effect that may be mediated.

pander(summary(lm(UVindex~Robot+Age.C,
                  data=UVindex)), 
       caption="Step 1: Show that the causal variable is correlated with the outcome")

#Step 2: Show that the causal variable is correlated with the mediator.  Use M as the criterion variable in the regression equation and X as a predictor (estimate and test path a).  This step essentially involves treating the mediator as if it were an outcome variable.

pander(summary(lm(Mind.C~Robot+Age.C,
                  data=UVindex)), 
       caption="Step 2: Show that the causal variable is correlated with the mediator")

#Step 3:  Show that the mediator affects the outcome variable.  Use Y as the criterion variable in a regression equation and X and M as predictors (estimate and test path b).  It is not sufficient just to correlate the mediator with the outcome because the mediator and the outcome may be correlated because they are both caused by the causal variable X.  Thus, the causal variable must be controlled in establishing the effect of the mediator on the outcome.

pander(summary(lm(UVindex~Mind.C+Robot+Age.C,
                  data=UVindex)), 
       caption="Step 3:  Show that the mediator affects the outcome variable")

#Step 4:  To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero (see discussion below on significance testing).   The effects in both Steps 3 and 4 are estimated in the same equation.
```

###MacKinnon Mediation

```{r MacKinnon_mediation_mind_age}
#MacKinnon et al., 2007
#Product of coefficients methods

#Estimate Equations 2 (Y = i + c'X + bM + e) and 3 (M = i + aX + e), i = intercept, e = error, M = mediator, X = predictor, y = criterion. Compute product of a & b.

#Equation 2
eq2 = lm(UVindex~Mind.C+Robot+Age.C,
         data=UVindex)
pander(summary(eq2), caption='Is the slope of Mind significantly different from zero?')
b = eq2$coefficients[["Mind.C"]]
var.b = (summary(eq2)$coef[,'Std. Error'][['Mind.C']])^2
n.b = length(resid(eq2))

lm.beta(lm(UVindex~Mind.C+Robot+Age.C,
         data=UVindex))

#Equation 3
eq3 = lm(Mind.C~Robot+Age.C,
         data=UVindex)
pander(summary(eq3), caption='Is the slope of Robot significantly different from zero?')
a = eq3$coefficients[["RobotKaspar Front"]]
var.a = (summary(eq3)$coef[,'Std. Error'][['RobotKaspar Front']])^2
n.a = length(resid(eq3))

lm.beta(lm(Mind.C~Robot+Age.C,
         data=UVindex))
#To test for significance, the product is then divided by the standard error of the product and the ratio is compared to a standard normal distribution.

#variance of the roduct of two independent random variables (Goodman, 1960)
var.ab = a^2*(var.b/n.b) + b^2*(var.a/n.a) - ((var.a/n.a)*(var.b/n.b)) 

#The standard error of an estimate may also be defined as the square root of the estimated error 
se.ab = sqrt(abs(var.ab))

sig = (a*b)/se.ab
abs(sig)>1.96

#The rationale behind this method is that mediation depends on the extent to which the program changes the mediator, a, and the extent to which the mediator affects the outcome variable, b.
```

The product of the coefficients ab = `r round(a*b,4)`, t = `r round(sig,2)` (MacKinnon, Fairchild, & Fritz, 2007)

```{r scatter_mediation_mind}
#where the independent variable, X, is dichotomous (to simplify the plot), the mediator, M, is on the horizontal axis, and the dependent variable, Y, is on the vertical axis

KasparFront = UVindex[ which( UVindex$Robot=="Kaspar Front"), ]
KasparBack = UVindex[ which( UVindex$Robot=="Kaspar Back"), ]

plot.new()
par(mar=c(5,5,5,5)+0.1,bg='white')
plot(KasparFront$Mind, 
     jitter(KasparFront$UVindex), 
     main = "Mediation by Attributions of Mind \n when controlling for age", 
     xlab = 'Mind', 
     ylab="Uncanniness", 
     yaxt = 'n', 
     xaxt = 'n', 
     col = humanColor, 
     cex.main = titleSizeSmall, 
     cex.axis = responseAxis, 
     cex.lab = responseAxis)

#add regression line for human-like condition
abline(lm(UVindex~Mind+Age,
          data = KasparFront),
       col = humanColor, 
       lwd = lineWidth)

#print x-axis labels
axis(1, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)
#print y-axis labels
axis(2, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)

#add data points for machine-Like condition to plot
points(jitter(KasparBack$Experience), jitter(KasparBack$UVindex), col = machineColor)
#add regression line for machine-like condition
abline(lm(UVindex~Mind+Age,
          data = KasparBack),
       col = machineColor, 
       lwd = lineWidth)

legend('topright',
       c("Human-like", "Machine-like"), 
       col = c(humanColor,machineColor), 
       lwd = lineWidth, 
       bg = 'white', 
       inset = .025)

#The two slanted lines in the plot represent the relation of M to Y in each X group, one line for the human-like robot group and one line for the machine-like robot group. The two lines are not parallel because there is an XM interaction in Equation 2, with the slope of each line equal to the b coefficient (b = -.0039 se(b) = .13; b = -.17; se(b) = .13). The distance between the horizontal lines in the plots is equal to the overall effect of X on Y, c (c = -.04, se(c) = .09) and the distance between the vertical lines is equal to the effect of X on M, a (a = .67, se(a) = .20). The mediated effect is the change in the regression line relating M to Y for a change in M of a units as shown in the graph. Plots of the mediated effect may be useful to investigate the distributions of data for outliers and to improve understanding of relations among variables in the mediation model.
```

##Mind as a mediator for robot when controlling for age for only older children

###Baron & Kenny Mediation
Step 1: Show that the causal variable is correlated with the outcome

Step 2: Show that the causal variable is correlated with the mediator

Step 3: Show that the mediator affects the outcome variable

Step 4: To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero

```{r BaronKenny_mediation_mind_old}
#Find a mediation test, where experience mediates the effect of robot type on Uncanniness while controlling for age.

#http://davidakenny.net/cm/mediate.htm
#Step 1:  Show that the causal variable is correlated with the outcome.  Use Y as the criterion variable in a regression equation and X as a predictor (estimate and test path c in the above figure). This step establishes that there is an effect that may be mediated.

pander(summary(lm(UVindex~Robot+Age.C,
                  data=UVindex.old)), 
       caption="Step 1: Show that the causal variable is correlated with the outcome")

#Step 2: Show that the causal variable is correlated with the mediator.  Use M as the criterion variable in the regression equation and X as a predictor (estimate and test path a).  This step essentially involves treating the mediator as if it were an outcome variable.

pander(summary(lm(Mind.C~Robot+Age.C,
                  data=UVindex.old)), 
       caption="Step 2: Show that the causal variable is correlated with the mediator")

#Step 3:  Show that the mediator affects the outcome variable.  Use Y as the criterion variable in a regression equation and X and M as predictors (estimate and test path b).  It is not sufficient just to correlate the mediator with the outcome because the mediator and the outcome may be correlated because they are both caused by the causal variable X.  Thus, the causal variable must be controlled in establishing the effect of the mediator on the outcome.

pander(summary(lm(UVindex~Mind.C+Robot+Age.C,
                  data=UVindex.old)), 
       caption="Step 3:  Show that the mediator affects the outcome variable")

#Step 4:  To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path c') should be zero (see discussion below on significance testing).   The effects in both Steps 3 and 4 are estimated in the same equation.


```

###MacKinnon Mediation

```{r MacKinnon_mediation_mind_old}
#MacKinnon et al., 2007
#Product of coefficients methods

#Estimate Equations 2 (Y = i + c'X + bM + e) and 3 (M = i + aX + e), i = intercept, e = error, M = mediator, X = predictor, y = criterion. Compute product of a & b.

#Equation 2
eq2 = lm(UVindex~Mind.C+Robot+Age.C,
         data=UVindex.old)
pander(summary(eq2), caption='Is the slope of Mind significantly different from zero?')
b = eq2$coefficients[["Mind.C"]]
var.b = (summary(eq2)$coef[,'Std. Error'][['Mind.C']])^2
n.b = length(resid(eq2))

#Equation 3
eq3 = lm(Mind.C~Robot+Age.C,
         data=UVindex.old)
pander(summary(eq3), caption='Is the slope of Robot significantly different from zero?')
a = eq3$coefficients[["RobotKaspar Front"]]
var.a = (summary(eq3)$coef[,'Std. Error'][['RobotKaspar Front']])^2
n.a = length(resid(eq3))

#To test for significance, the product is then divided by the standard error of the product and the ratio is compared to a standard normal distribution.

#variance of the roduct of two independent random variables (Goodman, 1960)
var.ab = a^2*(var.b/n.b) + b^2*(var.a/n.a) - ((var.a/n.a)*(var.b/n.b)) 

#The standard error of an estimate may also be defined as the square root of the estimated error 
se.ab = sqrt(abs(var.ab))

sig = (a*b)/se.ab
abs(sig)>1.96

#The rationale behind this method is that mediation depends on the extent to which the program changes the mediator, a, and the extent to which the mediator affects the outcome variable, b.
```

The product of the coefficients ab = `r round(a*b,4)`, t = `r round(sig,2)` (MacKinnon, Fairchild, & Fritz, 2007)

```{r scatter_mediation_mind_old}
#where the independent variable, X, is dichotomous (to simplify the plot), the mediator, M, is on the horizontal axis, and the dependent variable, Y, is on the vertical axis

KasparFront = UVindex.old[ which( UVindex.old$Robot=="Kaspar Front"), ]
KasparBack = UVindex.old[ which( UVindex.old$Robot=="Kaspar Back"), ]


#create scatter plot that looks at effect of age and robot type on creepiness factor
plot.new()
par(mar=c(5,5,5,5)+0.1)
plot(KasparFront$Mind, 
     jitter(KasparFront$UVindex), 
     main = "Mediation Plot", 
     xlab = 'Mind', 
     ylab="Uncanniness", 
     yaxt = 'n', 
     xaxt = 'n', 
     col = humanColor, 
     cex.main = titleSizeSmall, 
     cex.axis = responseAxis, 
     cex.lab = responseAxis)

#print x-axis labels
axis(1, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)
#print y-axis labels
axis(2, 
     at = c(1:4), 
     labels = c('Not at all', 'A little bit', 'A medium amount', 'A lot'), 
     cex.axis = responseAxis)


#add regression line for human-like condition
abline(lm(UVindex~Mind+Age, 
          data=KasparFront), 
       col = humanColor, 
       lwd = lineWidth)

#add data points for machine-Like condition to plot
points(jitter(KasparBack$Experience), jitter(KasparBack$UVindex), col = machineColor)
#add regression line for machine-like condition
abline(lm(UVindex~Mind+Age,
          data=KasparBack), 
       col = machineColor, 
       lwd = lineWidth)

legend('topright',
       c("Human-like", "Machine-like"), 
       col = c(humanColor,machineColor), 
       lwd = lineWidth, 
       bg = 'white', 
       inset = .025)

#The two slanted lines in the plot represent the relation of M to Y in each X group, one line for the human-like robot group and one line for the machine-like robot group. The two lines are not parallel because there is an XM interaction in Equation 2, with the slope of each line equal to the b coefficient (b = -.0039 se(b) = .13; b = -.17; se(b) = .13). The distance between the horizontal lines in the plots is equal to the overall effect of X on Y, c (c = -.04, se(c) = .09) and the distance between the vertical lines is equal to the effect of X on M, a (a = .67, se(a) = .20). The mediated effect is the change in the regression line relating M to Y for a change in M of a units as shown in the graph. Plots of the mediated effect may be useful to investigate the distributions of data for outliers and to improve understanding of relations among variables in the mediation model.
```

```{r power_analysis}
library(pwr)

#effect size Gray & Wegner
# mean difference divided by pooled SD
effect.size.uv = (1.77-1.32)/((.8+.57)/2) #uncanny valley effect

sd = ((.42-0)*sqrt(111))/4.3
effect.size.exp =  (.42-0)/sd #experience slope

#u = numerator degrees of freedom (p-1)
#v = denominator degrees of freedom (n-p)
#f2 = effect size (.02 - small, .15 - medium)
#sig.level = significance level (.05)

pwr.f2.test(u = 7-1, v= 240-7,f2 =effect.size.uv, sig.level = .05)

pwr.t.test(n = 89, d=effect.size.exp, sig.level = .05, type= "one.sample")
```

```{r diff_scores_nao}
UV.young = UV[which(UV$AgeGroup==1|UV$AgeGroup==2),]
UV.9to11 = UV[which(UV$AgeGroup==3),]
UV.old = UV[which(UV$AgeGroup==4),]

t.test(UV.young$UVindex.Back,UV.young$UVindex.Nao, paired = T)
cohensD(UV.young$UVindex.Back,UV.young$UVindex.Nao, method='paired')

t.test(UV.9to11$UVindex.Back,UV.9to11$UVindex.Nao, paired = T)
cohensD(UV.9to11$UVindex.Back,UV.9to11$UVindex.Nao, method='paired')

t.test(UV.old$UVindex.Back,UV.old$UVindex.Nao, paired = T)
cohensD(UV.old$UVindex.Back,UV.old$UVindex.Nao, method='paired')

```


```{r}
UV$UV.score[which(!is.na(UV$UVindex.Back))] = UV$UVindex.Back[which(!is.na(UV$UVindex.Back))]
UV$UV.score[which(!is.na(UV$UVindex.Front))] = UV$UVindex.Front[which(!is.na(UV$UVindex.Front))]

UV$Agency.C[which(!is.na(UV$Agency.Back.C))] = UV$Agency.Back.C[which(!is.na(UV$Agency.Back.C))]
UV$Agency.C[which(!is.na(UV$Agency.Front.C))] = UV$Agency.Front.C[which(!is.na(UV$Agency.Front.C))]

UV$Exp.C[which(!is.na(UV$Exp.Back.C))] = UV$Exp.Back.C[which(!is.na(UV$Exp.Back.C))]
UV$Exp.C[which(!is.na(UV$Exp.Front.C))] = UV$Exp.Front.C[which(!is.na(UV$Exp.Front.C))]

UV$Condition = ifelse(!is.na(UV$UVindex.Back),"Back","Front")
UV$Condition[which(is.na(UV$UV.score))] = NA

UV$Age2 = UV$Age.C^2

lm.out = lm(UV.score~UVindex.Nao.C+Condition*Age.C+Condition*Age2,
            data=UV)

plot(lm.out)

pander(summary(lm.out), caption = "full sample regression")

pander(lm.beta(lm.out), caption = "beta estimates of full sample regression")

```

```{r}
hist(UV$UV.score)

UV$diff = UV$UV.score-UV$UVindex.Nao

pander(Anova(lm(diff~Condition*AgeGroup2,
              data=UV), type='II'))

barByAge(v.back = UV$diff[which(UV$Condition=="Back")],v.front = UV$diff[which(UV$Condition=="Front")],UV,"")

lm.out = lm(diff~Condition*Age.C+Condition*Age2,
            data=UV)

plot(lm.out)

pander(summary(lm.out), caption = "full sample regression")
```

```{r}
UV$Age2 = UV$Age.C^2
UV$Age3 = UV$Age.C^3

lm.out = lm(UV.score~Condition*Age.C+Condition*Age2+Condition*Age3+UVindex.Nao.C,
            data=UV)

pander(summary(lm.out), caption = "full sample regression")

lm.out = lm(diff~Condition*Age.C+Condition*Age2+Condition*Age3,
            data=UV)

pander(summary(lm.out), caption = "full sample regression")

plot(lm.out)

lm.out = lm(diff~Agency.C*Age.C+Agency.C*Age2,
            data=UV)
pander(summary(lm.out), caption = "full sample regression")

lm.out = lm(diff~Agency.C*Age.C+Condition*Age.C,
            data=UV)
lm.beta(lm.out)
pander(summary(lm.out), caption = "full sample regression")

lm.out = lm(diff~Exp.C*Age.C+Condition*Age.C,
            data=UV[which(UV$Age>mean(UV$Age)),])
pander(summary(lm.out), caption = "full sample regression")

lm.out = lm(diff~Agency.C*Age.C+Condition*Age.C,data=UV)
pander(summary(lm.out), caption = "full sample regression")

```

```{r}
library(interplot)


m_cond = lm(diff~Age.C*Condition+I(Age.C^2)*Condition+I(Age.C^3)*Condition,data=UV)
summary(m_cond)
interplot(m = m_cond, var1 = "Condition", var2 = "Age.C")


m_agency = lm(diff~Age.C*Agency.C+Condition,data=UV)
summary(m_agency)
interplot(m = m_agency, var1 = "Agency.C", var2 = "Age.C")
```

```{r}
UV$d.back = UV$UVindex.Back-UV$UVindex.Nao
UV$d.front = UV$UVindex.Front-UV$UVindex.Nao

barByAge(UV$d.back,UV$d.front,UV,"diff")
```

